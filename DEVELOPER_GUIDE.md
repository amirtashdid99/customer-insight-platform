# ðŸŽ¯ Customer Insight Platform - Step-by-Step Implementation Guide

## Overview
This guide will walk you through implementing the Full-Stack Customer Insight Platform from scratch. Each step builds on the previous one, and you'll learn the WHY behind each decision.

---

## ðŸ“‹ Prerequisites Checklist

Before starting, make sure you have:

- [ ] Python 3.9 or higher installed
- [ ] PostgreSQL 14+ installed and running
- [ ] Node.js 16+ and npm installed
- [ ] Git installed
- [ ] A code editor (VS Code recommended)
- [ ] Basic knowledge of Python, JavaScript, and SQL

---

## Phase 1: Database & ML Models Setup (Days 1-3)

### Step 1.1: Set Up PostgreSQL Database

**What to do:**

1. **Install PostgreSQL** (if not already installed):
   - Windows: Download from postgresql.org
   - Verify installation: Open PowerShell and run `psql --version`

2. **Create the database:**
   ```powershell
   # Connect to PostgreSQL
   psql -U postgres
   
   # In psql prompt:
   CREATE DATABASE customer_insight_db;
   \q
   ```

3. **Configure environment:**
   - Copy `backend\.env.example` to `backend\.env`
   - Edit `.env` and update the DATABASE_URL with your password:
     ```
     DATABASE_URL=postgresql://postgres:YOUR_PASSWORD@localhost:5432/customer_insight_db
     ```
   - Generate a secure SECRET_KEY (at least 32 characters)

**Why this matters:**
- PostgreSQL is production-grade and handles complex queries
- Proper env configuration keeps secrets out of code
- This separates development from production settings

---

### Step 1.2: Set Up Python Virtual Environment

**What to do:**

```powershell
cd backend

# Create virtual environment
python -m venv venv

# Activate it
venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

**Why this matters:**
- Virtual environments isolate project dependencies
- Prevents conflicts with other Python projects
- Makes the project reproducible on any machine

**What makes this unique:**
- We're using latest versions of transformers and XGBoost
- Custom combination of web scraping + ML + API in one stack

---

### Step 1.3: Download Training Dataset

**What to do:**

1. Go to [Kaggle Telco Churn Dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
2. Download `WA_Fn-UseC_-Telco-Customer-Churn.csv`
3. Place it in `ml_training/datasets/` directory

**Why this matters:**
- Real-world dataset with actual customer churn patterns
- ~7000 samples - enough for solid model training
- Industry-standard benchmark dataset

---

### Step 1.4: Train the Churn Prediction Model

**What to do:**

```powershell
cd ml_training\scripts
python train_churn_model.py
```

**What happens:**
- Loads and preprocesses the Telco dataset
- Handles missing values and encodes categorical features
- Trains an XGBoost classifier with optimized hyperparameters
- Performs cross-validation
- Saves model, scaler, and encoders to `trained_models/`
- Displays feature importance

**Expected output:**
```
ROC-AUC Score: 0.84+
Cross-validation ROC-AUC: 0.83+
Top features: tenure, TotalCharges, MonthlyCharges, etc.
```

**Why this approach is unique:**
- Custom preprocessing pipeline you built
- We adapt the model for sentiment-based churn prediction (creative!)
- Feature importance analysis helps understand what drives churn

---

### Step 1.5: Test Sentiment Analysis Model

**What to do:**

Create a test script `backend/test_sentiment.py`:

```python
from app.ml.sentiment_analyzer import get_sentiment_analyzer

# Initialize analyzer (downloads model on first run)
analyzer = get_sentiment_analyzer()

# Test different sentiments
test_texts = [
    "This product is amazing! Best purchase ever!",
    "Terrible experience. Would not recommend.",
    "It's okay, nothing special."
]

for text in test_texts:
    result = analyzer.analyze(text)
    print(f"Text: {text}")
    print(f"Result: {result}\n")
```

Run it:
```powershell
python test_sentiment.py
```

**What happens:**
- Downloads DistilBERT model from Hugging Face (~250MB)
- Runs sentiment analysis on test texts
- Shows sentiment, score, and confidence

**Why this is unique:**
- Using state-of-the-art transformer model (not basic VADER or TextBlob)
- Batch processing capability for efficiency
- Customized output format for our use case

---

### Step 1.6: Initialize Database Schema

**What to do:**

```powershell
cd backend

# Initialize Alembic (database migrations)
alembic revision --autogenerate -m "Initial schema"

# Apply migration to create tables
alembic upgrade head
```

**What happens:**
- Alembic detects your SQLAlchemy models
- Creates migration scripts
- Creates tables: products, analyses, customer_comments, topics

**Verify:**
```powershell
psql -U postgres -d customer_insight_db
\dt  # List tables
\d products  # Describe products table
```

**Why this matters:**
- Database migrations track schema changes over time
- Easy to roll back if something goes wrong
- Essential for team collaboration

**What's unique about our schema:**
- Normalized design with proper relationships
- Enum types for status and sentiment (type-safe)
- Timestamps for time-series analysis
- Supports multiple analyses per product

---

## Phase 2: Backend API Development (Days 4-6)

### Step 2.1: Test the FastAPI Application

**What to do:**

```powershell
cd backend\app
python main.py
```

**Or using uvicorn directly:**
```powershell
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**What happens:**
- FastAPI server starts on http://localhost:8000
- Interactive API docs at http://localhost:8000/docs
- Database tables are created if they don't exist

**Test the API:**
1. Open http://localhost:8000/docs in browser
2. Try the `/health` endpoint - should return `{"status": "healthy"}`
3. Explore the API documentation

**What's unique:**
- Auto-generated interactive API docs (Swagger UI)
- Async/await for high performance
- Pydantic models ensure type safety

---

### Step 2.2: Test the Analysis Endpoint

**What to do:**

Using the Swagger UI at http://localhost:8000/docs:

1. Find `POST /api/analysis/analyze`
2. Click "Try it out"
3. Enter a product name: `"iPhone 15"`
4. Click "Execute"

**What happens:**
- API creates a product record
- Creates an analysis job
- Starts background scraping and analysis
- Returns immediately with analysis_id

**Check the status:**
1. Copy the `analysis_id` from response
2. Use `GET /api/analysis/status/{analysis_id}`
3. Watch status change: PENDING â†’ IN_PROGRESS â†’ COMPLETED

**What's unique about this design:**
- Async background processing (doesn't block the API)
- Real-time status updates
- Fault-tolerant error handling

---

### Step 2.3: Test the Dashboard Endpoint

**What to do:**

Once analysis is COMPLETED:

1. Use `GET /api/analysis/dashboard/{product_name}`
2. Enter: `"iPhone 15"`
3. Execute

**What you'll see:**
- Complete analysis results
- Sentiment distribution (% positive/negative/neutral)
- Recent comments with sentiment scores
- Extracted topics with mention counts
- Churn risk score and level

**Explore the data:**
- Notice how different sources (Reddit, Twitter, reviews) are mixed
- Check which topics were extracted (price, quality, support, etc.)
- See how sentiment scores range from -1 to 1

**What's unique:**
- Multi-source data aggregation
- Topic extraction without manual labeling
- Risk level classification

---

### Step 2.4: Understanding the Data Pipeline

**Let's trace one analysis from start to finish:**

1. **Scraping** (`app/scrapers/web_scraper.py`):
   - Simulates scraping from Reddit, Twitter, review sites
   - In production, you'd use actual APIs or web scraping
   - Returns Comment objects with text, source, author, etc.

2. **Sentiment Analysis** (`app/ml/sentiment_analyzer.py`):
   - Batch processes all comments through DistilBERT
   - Returns sentiment type, score (-1 to 1), confidence
   - Handles errors gracefully

3. **Churn Prediction** (`app/ml/churn_predictor.py`):
   - Uses aggregate sentiment metrics
   - Custom formula: considers avg sentiment, negative ratio, volatility
   - Returns churn probability (0 to 1) and risk level

4. **Topic Extraction** (`app/api/analysis.py` - extract_topics function):
   - Keyword matching for common themes
   - Counts mentions per topic
   - Calculates average sentiment per topic

5. **Database Storage**:
   - Saves all comments, topics, and metrics
   - Enables historical analysis and trending

**What makes this unique:**
- End-to-end pipeline in one request
- Combines multiple AI techniques
- Production-ready error handling

---

### Step 2.5: Customize the Scraper (Make it Yours!)

**Current state:** The scraper uses synthetic data for demo purposes.

**To make it production-ready:**

1. **Option A: Use Reddit API**
   ```python
   # Install: pip install praw
   import praw
   
   reddit = praw.Reddit(
       client_id="YOUR_CLIENT_ID",
       client_secret="YOUR_SECRET",
       user_agent="CustomerInsightPlatform/1.0"
   )
   
   # Search for product mentions
   for submission in reddit.subreddit("all").search(product_name, limit=50):
       # Process submission.title and submission.selftext
   ```

2. **Option B: Use Twitter API v2**
   ```python
   # Install: pip install tweepy
   import tweepy
   
   client = tweepy.Client(bearer_token="YOUR_BEARER_TOKEN")
   tweets = client.search_recent_tweets(
       query=product_name,
       max_results=100
   )
   ```

3. **Option C: Web Scraping with BeautifulSoup**
   ```python
   # For review sites like Trustpilot
   async with aiohttp.ClientSession() as session:
       async with session.get(url) as response:
           html = await response.text()
           soup = BeautifulSoup(html, 'html.parser')
           reviews = soup.find_all('div', class_='review-content')
   ```

**Why keep it synthetic for now:**
- No API keys needed to demo
- Faster testing and development
- Shows the concept clearly
- Easy to replace later

**What makes your implementation unique:**
- Multi-source aggregation strategy
- Async concurrent scraping
- Configurable limits and timeouts

---

## Phase 3: Frontend Development (Days 7-10)

### Step 3.1: Initialize React Application

**What to do:**

```powershell
cd ..  # Back to project1 root
npx create-react-app frontend --template typescript
cd frontend
```

**Install additional dependencies:**
```powershell
npm install axios recharts react-router-dom
npm install -D @types/react-router-dom tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

**Why these choices:**
- TypeScript for type safety (catches bugs early)
- Recharts for beautiful data visualization
- Tailwind CSS for rapid UI development
- Axios for API calls

---

### Step 3.2: Configure Tailwind CSS

**Edit `tailwind.config.js`:**

```javascript
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./src/**/*.{js,jsx,ts,tsx}",
  ],
  theme: {
    extend: {
      colors: {
        primary: '#3b82f6',
        success: '#10b981',
        warning: '#f59e0b',
        danger: '#ef4444',
      }
    },
  },
  plugins: [],
}
```

**Add to `src/index.css`:**

```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

---

### Step 3.3: Create API Service

**Create `src/services/api.ts`:**

```typescript
import axios from 'axios';

const API_BASE_URL = 'http://localhost:8000';

const api = axios.create({
  baseURL: API_BASE_URL,
  headers: {
    'Content-Type': 'application/json',
  },
});

export interface AnalysisRequest {
  product_name: string;
}

export interface AnalysisResponse {
  message: string;
  analysis_id: number;
  status: string;
  estimated_time_seconds: number;
}

export interface DashboardData {
  product: {
    id: number;
    name: string;
    created_at: string;
  };
  latest_analysis: {
    id: number;
    status: string;
    total_comments: number;
    avg_sentiment_score: number;
    positive_count: number;
    negative_count: number;
    neutral_count: number;
    churn_risk_score: number;
    completed_at: string;
  } | null;
  recent_comments: Array<{
    id: number;
    text: string;
    source: string;
    sentiment: string;
    sentiment_score: number;
    confidence: number;
  }>;
  topics: Array<{
    id: number;
    name: string;
    mention_count: number;
    avg_sentiment: number;
  }>;
  sentiment_distribution: {
    positive: number;
    negative: number;
    neutral: number;
  };
  risk_level: string | null;
}

export const apiService = {
  startAnalysis: (data: AnalysisRequest) =>
    api.post<AnalysisResponse>('/api/analysis/analyze', data),
  
  getAnalysisStatus: (analysisId: number) =>
    api.get(`/api/analysis/status/${analysisId}`),
  
  getDashboardData: (productName: string) =>
    api.get<DashboardData>(`/api/analysis/dashboard/${productName}`),
};
```

---

### Step 3.4: Create Dashboard Components

This will be in the next message as we have more to cover...

**What you've accomplished so far:**

âœ… Database set up and configured
âœ… ML models trained and tested
âœ… Backend API fully functional
âœ… Tested the complete analysis pipeline
âœ… Frontend initialized and configured

**What makes this project unique for your portfolio:**

1. **Custom ML Pipeline**: Not just using off-the-shelf solutions
2. **Multi-Model Integration**: Combines sentiment analysis + churn prediction
3. **Real-World Architecture**: Production-ready patterns (async, migrations, etc.)
4. **End-to-End Ownership**: From data to deployment
5. **Scalable Design**: Easy to add more data sources or models

---

## Next Steps

Continue to the Frontend Development section to complete the UI...

